# Hadoop/Spark Integration Plan for FactCheck Platform

## ðŸŽ¯ Tá»•ng quan

TÃ­ch há»£p Hadoop/Spark vÃ o FactCheck Platform Ä‘á»ƒ xá»­ lÃ½ big data analytics, machine learning, vÃ  batch processing cho fact-checking tá»± Ä‘á»™ng.

## ðŸ—ï¸ Kiáº¿n trÃºc TÃ­ch há»£p

### Current Architecture (Real-time)
- âœ… 7 Microservices vá»›i Event-Driven Architecture
- âœ… Firestore cho real-time data
- âœ… Redis/RabbitMQ cho event streaming
- âœ… Event Sourcing Ä‘Ã£ implement

### New Big Data Layer
```
services/
â”œâ”€â”€ spark-service/           # Port 3010 - Spark job management
â”œâ”€â”€ etl-service/            # Port 3011 - Data pipeline
â”œâ”€â”€ analytics-service/      # Port 3012 - Analytics API
â””â”€â”€ scheduler-service/      # Port 3013 - Job scheduling
```

## ðŸ“Š Use Cases cho Hadoop/Spark

### 1. **Fake News Detection**
```javascript
// Spark ML Pipeline
const fakeNewsDetection = {
  input: "news articles from News Service",
  processing: [
    "Text preprocessing",
    "Feature extraction (TF-IDF, Word2Vec)",
    "ML models (Random Forest, SVM)",
    "Sentiment analysis"
  ],
  output: "Credibility score + classification"
};
```

### 2. **Link Pattern Analysis**
```javascript
// Batch analysis cá»§a link patterns
const linkAnalysis = {
  input: "Historical link data from Link Service",
  processing: [
    "URL pattern mining",
    "Domain reputation analysis", 
    "Phishing pattern detection",
    "Network analysis"
  ],
  output: "Risk patterns + blacklist updates"
};
```

### 3. **Community Behavior Analytics**
```javascript
// User behavior analysis
const communityAnalytics = {
  input: "Posts, votes, comments from Community Service",
  processing: [
    "User clustering",
    "Influence analysis",
    "Spam detection",
    "Trend analysis"
  ],
  output: "User insights + moderation recommendations"
};
```

## ðŸ”§ Technical Implementation

### Environment Variables (.env additions)
```env
# =============================================================================
# BIG DATA SERVICES
# =============================================================================
SPARK_SERVICE_PORT=3010
ETL_SERVICE_PORT=3011
ANALYTICS_SERVICE_PORT=3012
SCHEDULER_SERVICE_PORT=3013

SPARK_SERVICE_URL=http://localhost:3010
ETL_SERVICE_URL=http://localhost:3011
ANALYTICS_SERVICE_URL=http://localhost:3012
SCHEDULER_SERVICE_URL=http://localhost:3013

# =============================================================================
# HADOOP/SPARK CONFIGURATION
# =============================================================================
SPARK_MASTER_URL=spark://localhost:7077
HADOOP_HOME=/opt/hadoop
SPARK_HOME=/opt/spark
HDFS_NAMENODE_URL=hdfs://localhost:9000

# Data Storage
HDFS_DATA_PATH=/factcheck/data
SPARK_WAREHOUSE_DIR=/factcheck/warehouse

# Processing Configuration
SPARK_EXECUTOR_MEMORY=2g
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_CORES=2
BATCH_PROCESSING_INTERVAL=1h
```

### Service Structure
```
services/spark-service/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ jobs/              # Spark job definitions
â”‚   â”‚   â”œâ”€â”€ fakeNewsDetection.js
â”‚   â”‚   â”œâ”€â”€ linkAnalysis.js
â”‚   â”‚   â””â”€â”€ communityAnalytics.js
â”‚   â”œâ”€â”€ ml/                # ML models
â”‚   â”‚   â”œâ”€â”€ textClassifier.py
â”‚   â”‚   â”œâ”€â”€ linkScorer.py
â”‚   â”‚   â””â”€â”€ userClustering.py
â”‚   â”œâ”€â”€ api/               # REST API
â”‚   â”‚   â”œâ”€â”€ jobs.js
â”‚   â”‚   â”œâ”€â”€ results.js
â”‚   â”‚   â””â”€â”€ health.js
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ spark.js
â”‚       â””â”€â”€ hadoop.js
â”œâ”€â”€ python/                # Python Spark jobs
â”œâ”€â”€ scala/                 # Scala Spark jobs (optional)
â””â”€â”€ docker/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ docker-compose.spark.yml
```

## ðŸš€ Implementation Phases

### Phase 1: Infrastructure Setup (Week 1-2)
1. **Docker containers** cho Hadoop/Spark
2. **Spark Service** cÆ¡ báº£n (Port 3010)
3. **ETL Service** cho data export tá»« Firestore
4. **Basic job scheduling**

### Phase 2: Data Pipeline (Week 3-4)
1. **Firestore â†’ HDFS** data pipeline
2. **Event streaming** tá»« EventBus â†’ Spark Streaming
3. **Batch processing** jobs
4. **Data validation** vÃ  quality checks

### Phase 3: ML & Analytics (Week 5-6)
1. **Fake news detection** model
2. **Link analysis** algorithms
3. **Community analytics** dashboard
4. **Real-time scoring** integration

### Phase 4: Integration (Week 7-8)
1. **API integration** vá»›i existing services
2. **Results feedback** vÃ o Firestore
3. **Performance optimization**
4. **Monitoring** vÃ  alerting

## ðŸ“ˆ Data Flow

### Batch Processing Flow
```
Firestore â†’ ETL Service â†’ HDFS â†’ Spark Jobs â†’ Results â†’ Analytics Service â†’ API Gateway
```

### Real-time Processing Flow  
```
EventBus â†’ Spark Streaming â†’ ML Models â†’ Real-time Scores â†’ Services
```

### Feedback Loop
```
Analytics Results â†’ Firestore â†’ Services â†’ Improved Decisions
```

## ðŸ” Monitoring & Observability

### Metrics to Track
- **Job execution times**
- **Data processing volumes**
- **ML model accuracy**
- **Resource utilization**
- **Error rates**

### Integration vá»›i existing monitoring
- **Prometheus** metrics tá»« Spark services
- **Grafana** dashboards cho big data metrics
- **Event-driven** alerts qua EventBus

## ðŸ’¡ Benefits

### For FactCheck Platform
1. **Automated fact-checking** vá»›i ML models
2. **Pattern detection** cho phishing/scam links
3. **User behavior insights** cho community moderation
4. **Scalable processing** cho large datasets
5. **Historical analysis** vÃ  trend detection

### Technical Benefits
1. **Separation of concerns** - Real-time vs Batch processing
2. **Scalability** - Handle growing data volumes
3. **Flexibility** - Easy to add new analytics
4. **Cost efficiency** - Batch processing cho heavy computations

## ðŸš¨ Considerations

### Challenges
1. **Complexity** - Additional infrastructure
2. **Data consistency** - Firestore â†” HDFS sync
3. **Resource usage** - Memory/CPU intensive
4. **Development overhead** - Multiple languages (JS/Python/Scala)

### Mitigation Strategies
1. **Start small** - Implement core use cases first
2. **Gradual migration** - Keep existing services unchanged
3. **Fallback mechanisms** - Handle Spark failures gracefully
4. **Resource limits** - Configure appropriate limits

## ðŸŽ¯ Success Metrics

### Technical KPIs
- **Job success rate** > 95%
- **Processing latency** < 30 minutes for batch jobs
- **Data freshness** < 1 hour lag
- **System availability** > 99.5%

### Business KPIs  
- **Fake news detection accuracy** > 85%
- **False positive rate** < 10%
- **Processing cost** reduction vs manual review
- **User satisfaction** with automated recommendations
